[
  {
    "id": 1,
    "feedId": 1,
    "title": "Ladder: Self-Improving LLMs Through Recursive Problem Decomposition",
    "url": "https://arxiv.org/abs/2503.00735",
    "content": "Computer Science > Machine Learning\n    \n\n    \n      arXiv:2503.00735 (cs)\n    \n\n\n  \n    \n  [Submitted on 2 Mar 2025 (v1), last revised 5 Mar 2025 (this version, v3)]\n    Title:LADDER: Self-Improving LLMs Through Recursive Problem Decomposition\n    Authors:Toby Simonds, Akira Yoshiyama            View a PDF of the paper titled LADDER: Self-Improving LLMs Through Recursive Problem Decomposition, by Toby Simonds and 1 other authors\n    View PDF\n    HTML (experimental)\n\n\n\n    \n            Abstract:We introduce LADDER (Learning through Autonomous Difficulty-Driven Example Recursion), a framework which enables Large Language Models to autonomously improve their problem-solving capabilities through self-guided learning by recursively generating and solving progressively simpler variants of complex problems. Unlike prior approaches that require curated datasets or human feedback, LADDER leverages a model's own capabilities to generate easier question variants. We demonstrate LADDER's effectiveness in the subject of mathematical integration, improving Llama 3.2 3B's accuracy from 1% to 82% on undergraduate-level problems and enabling Qwen2.5 7B Deepseek-R1 Distilled to achieve 73% on the MIT Integration Bee qualifying examination. We also introduce TTRL (Test-Time Reinforcement Learning), where we perform reinforcement learning on variants of test problems at inference time. TTRL enables Qwen2.5 7B Deepseek-R1 Distilled to achieve a state-of-the-art score of 90% on the MIT Integration Bee qualifying examination, surpassing OpenAI o1's performance. These results show how self-directed strategic learning can achieve significant capability improvements without relying on architectural scaling or human supervision.\n    \n\n    \n    \n      \n          Subjects:\n          \n            Machine Learning (cs.LG); Artificial Intelligence (cs.AI)\n        \n          Cite as:\n          arXiv:2503.00735 [cs.LG]\n        \n        \n           \n          (or \n              arXiv:2503.00735v3 [cs.LG] for this version)\n          \n        \n        \n           \n                        https://doi.org/10.48550/arXiv.2503.00735\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Akira Yoshiyama [view email]                  [v1]\n        Sun, 2 Mar 2025 05:16:43 UTC (286 KB)\n            [v2]\n        Tue, 4 Mar 2025 14:30:32 UTC (203 KB)\n    [v3]\n        Wed, 5 Mar 2025 11:50:24 UTC (203 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled LADDER: Self-Improving LLMs Through Recursive Problem Decomposition, by Toby Simonds and 1 other authorsView PDFHTML (experimental)TeX SourceOther Formats\n      \n          \n          view license\n        \n    \n        \n    Current browse context: cs.LG\n\n  \n\n      < prev\n    \n      |      \n      next >\n    \n  \n    new\n     | \n    recent\n     | 2025-03\n  \n    Change to browse by:\n    \n        cs\n        cs.AI\n    \n  \n\n    \n      \n        References & Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    a\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            ×\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n                    \n            \n              \n                \n                \n                IArxiv recommender toggle\n              \n            \n            \n              IArxiv Recommender\n              (What is IArxiv?)\n            \n          \n\n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs\n      \n      \n        \n          \n            arXivLabs: experimental projects with community collaborators\n            arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n            Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n            Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n          \n          \n            \n          \n        \n      \n\n    \n\n\n  \n    Which authors of this paper are endorsers? |\n    Disable MathJax (What is MathJax?)",
    "published": "2025-03-07T06:45:57.000Z",
    "hasSummary": true,
    "summary": "LADDER: Self-Improving LLMs Through Recursive Problem Decomposition is a framework that enables Large Language Models to autonomously improve their problem-solving capabilities by recursively generating and solving progressively simpler variants of complex problems. The authors introduce two key components - LADDER, which leverages the model's own capabilities to generate easier question variants, and TTRL (Test-Time Reinforcement Learning), where reinforcement learning is performed on test problem variants at inference time.\n\nThe framework is demonstrated in the domain of mathematical integration, improving the accuracy of an Llama 3.2 3B from 1% to 82% on undergraduate-level problems and enabling Qwen2.5 7B Deepseek-R1 Distilled to achieve a score of 73% on MIT's Integration Bee qualifying examination. When using TTRL, Qwen2.5 7B Deepseek-R1 Distilled surpassed OpenAI o1's performance in achieving a state-of-the-art score of 90% on the same test.\n\nThis approach shows that self-directed strategic learning can lead to significant capability improvements without relying on architectural scaling or human supervision."
  },
  {
    "id": 2,
    "feedId": 1,
    "title": "Mistral OCR",
    "url": "https://mistral.ai/fr/news/mistral-ocr",
    "content": "Mistral OCRIntroducing the world’s best document understanding API.ResearchMar 6, 2025Mistral AI TeamThroughout history, advancements in information abstraction and retrieval have driven human progress. From hieroglyphs to papyri, the printing press to digitization, each leap has made human knowledge more accessible and actionable, fueling further innovation. \nToday, we’re at the precipice of the next big leap—to unlock the collective intelligence of all digitized information. Approximately 90% of the world’s organizational data is stored as documents, and to harness this potential, we are introducing Mistral OCR. Mistral OCR is an Optical Character Recognition API that sets a new standard in document understanding. Unlike other models, Mistral OCR comprehends each element of documents—media, text, tables, equations—with unprecedented accuracy and cognition. It takes images and PDFs as input and extracts content in an ordered interleaved text and images.\nAs a result, Mistral OCR is an ideal model to use in combination with a RAG system taking multimodal documents (such as slides or complex PDFs) as input.\nWe have made Mistral OCR as the default model for document understanding across millions of users on Le Chat, and are releasing the API mistral-ocr-latest at 1000 pages / $ (and approximately double the pages per dollar with batch inference). The API is available today on our developer suite la Plateforme, and coming soon to our cloud and inference partners, as well as on-premises.\nHighlights\n\n\nState of the art understanding of complex documents\n\n\nNatively multilingual and multimodal\n\n\nTop-tier benchmarks\n\n\nFastest in its category\n\n\nDoc-as-prompt, structured output\n\n\nSelectively available to self-host for organizations dealing with highly sensitive or classified information\n\n\nLet’s dive into each. \nState of the art understanding of complex documents\nMistral OCR excels in understanding complex document elements, including interleaved imagery, mathematical expressions, tables, and advanced layouts such as LaTeX formatting. The model enables deeper understanding of rich documents such as scientific papers with charts, graphs, equations and figures. \nBelow is an example of the model extracting text as well as imagery from a given PDF into a markdown file. You can access the notebook here. \n\nBelow we have side-by-side comparisons of PDFs and their respective OCR's outputs. Hover the slider  to switch between input and output. \n\n\n\nTables + Figures\n\n\n\n\n\nOCR result\n\n\n\n \n\n\n\n\nMath\n\n\n\n\n\nOCR result\n\n\n\n \n\n\n\n\nHindi\n\n\n\n\n\nOCR result\n\n\n\n \n\n\n\n\nDocument\n\n\n\n\n\nOCR result\n\n\n\n \n\n\n\n\nArabic\n\n\n\n\n\nOCR result\n\n\n\n \n\nTop-tier benchmarks\nMistral OCR has consistently outperformed other leading OCR models in rigorous benchmark tests. Its superior accuracy across multiple aspects of document analysis is illustrated below. We extract embedded images from documents along with text. The other LLMs compared below, do not have that capability. For a fair comparison, we evaluate them on our internal “text-only” test-set containing various publication papers, and PDFs from the web; below:\n\n       \n\n\nModel\nOverall\nMath\nMultilingual\nScanned\nTables\n\n\n\n\nGoogle Document AI\n83.42\n80.29\n86.42\n92.77\n78.16\n\n\nAzure OCR\n89.52\n85.72\n87.52\n94.65\n89.52\n\n\nGemini-1.5-Flash-002\n90.23\n89.11\n86.76\n94.87\n90.48\n\n\nGemini-1.5-Pro-002\n89.92\n88.48\n86.33\n96.15\n89.71\n\n\nGemini-2.0-Flash-001\n88.69\n84.18\n85.80\n95.11\n91.46\n\n\nGPT-4o-2024-11-20\n89.77\n87.55\n86.00\n94.58\n91.70\n\n\nMistral OCR 2503\n94.89\n94.29\n89.55\n98.96\n96.12\n\n\n\n\nNatively multilingual\nSince Mistral’s founding, we have aspired to serve the world with our models, and consequently strived for multilingual capabilities across our offerings. Mistral OCR takes this to a new level, being able to parse, understand, and transcribe thousands of scripts, fonts, and languages across all continents. This versatility is crucial for both global organizations that handle documents from diverse linguistic backgrounds, as well as hyperlocal businesses serving niche markets.\n\n   \n\n\nModel\nFuzzy Match in Generation\n\n\n\n\nGoogle-Document-AI\n95.88\n\n\nGemini-2.0-Flash-001\n96.53\n\n\nAzure OCR\n97.31\n\n\nMistral OCR\n99.02\n\n\n\n\n \nBenchmarks by language:\n\n     \n\n\nLanguage\nAzure OCR\nGoogle Doc AI\nMistral OCR\n\n\n\n\nru\n97.35\n95.56\n99.09\n\n\nfr\n97.50\n96.36\n99.20\n\n\nhi\n96.45\n95.65\n97.55\n\n\nzh\n91.40\n90.89\n97.11\n\n\npt\n97.96\n96.24\n99.42\n\n\nde\n98.39\n97.09\n99.51\n\n\nes\n98.54\n97.52\n99.54\n\n\ntr\n95.91\n93.85\n97.00\n\n\nuk\n97.81\n96.24\n99.29\n\n\nit\n98.31\n97.69\n99.42\n\n\nro\n96.45\n95.14\n98.79\n\n\n\n\n\nFastest in its category\nBeing lighter weight than most models in the category, Mistral OCR performs significantly faster than its peers, processing up to 2000 pages per minute on a single node. The ability to rapidly process documents ensures continuous learning and improvement even for high-throughput environments.\nDoc-as-prompt, structured output\nMistral OCR also introduces the use of documents as prompts, enabling more powerful and precise instructions. This capability allows users to extract specific information from documents and format it in structured outputs, such as JSON. Users can chain extracted outputs into downstream function calls and build agents.\nAvailable to self-host on a selective basis\nFor organizations with stringent data privacy requirements, Mistral OCR offers a self-hosting option. This ensures that sensitive or classified information remains secure within your own infrastructure, providing compliance with regulatory and security standards. If you would like to explore self-deployment with us, please let us know.\nUse cases\nWe are empowering our beta customers to elevate their organizational knowledge by transforming their extensive document repositories into actions and solutions. Some of the key use cases where our technology is making a significant impact include:\nDigitizing scientific research: Leading research institutions have been experimenting with Mistral OCR to convert scientific papers and journals into AI-ready formats, making them accessible to downstream intelligence engines. This has facilitated measurably faster collaboration and accelerated scientific workflows.\nPreserving historical and cultural heritage: Organizations and nonprofits that are custodians of heritage have been using Mistral OCR to digitize historical documents and artifacts, ensuring their preservation and making them accessible to a broader audience.\nStreamlining customer service: Customer service departments are exploring Mistral OCR to transform documentation and manuals into indexed knowledge, reducing response times and improving customer satisfaction.\nMaking literature across design, education, legal, etc. AI ready: Mistral OCR has also been helping companies convert technical literature, engineering drawings, lecture notes, presentations, regulatory filings and much more into indexed, answer-ready formats, unlocking intelligence and productivity across millions of documents.\nExperience it today\nMistral OCR capabilities are free to try on le Chat. To try the API, head over to la Plateforme. We’d love to get your feedback; expect the model to continue to get even better in the weeks to come. As part of our strategic engagement programs, we will also offer on-premises deployment on a selective basis.PartagerPlus de ressourcesNews\n  \n  \n  \n  \n  \nModèles\n  \n  \n  \n  \n  \nServices AI\n  \n  \n  \n  \n  \nLe prochain chapitre de l'IA est le vôtre.\n Essayez Le Chat\n  \n  \n  \n  \n  \nConstruire avec La Plateforme\n  \n  \n  \n  \n  \nParler à un expert",
    "published": "2025-03-06T17:39:39.000Z",
    "hasSummary": true,
    "summary": "Mistral OCR est une API d'intelligence artificielle pour la compréhension des documents. Elle permet de comprendre chaque élément des documents, tels que les médias, le texte, les tableaux et les équations avec une précision sans précédent. Les images et fichiers PDF peuvent être mis en œuvre comme entrée et extrait du contenu sous forme d'un texte ordonné et d'images.\n\nMistral OCR est conçu pour travailler avec un système de recherche par mots-clés (RAG) traitant des documents multimodaux, tels que les diapositives ou PDF complexes. Le modèle a été déployé sur la plateforme de développement de millions d'utilisateurs de Le Chat et est disponible à partir de 1000 pages pour $1 avec un doublementapproximativement des pages par dollar avec l'inertie en lots.\n\nLes performances du modèle ont été testées contre d'autres modèles OCR, avec une performance supérieure dans plusieurs domaines. Le modèle peut également gérer différents scripts, lettres et langues, ce qui est crucial pour les organisations internationales traitant de documents provenant de diverses langues et le commerce hyperlocal servant des marchés spécifiques.\n\nLe modèle est plus rapide que la plupart de ses concurrents et permet un traitement d'environ 2000 pages par minute sur une seule unité. Il offre également la fonctionnalité de document comme prompt, permettant des instructions précises et une sortie structurée en tant qu'entrée JSON.\n\nL'utilisation de Mistral OCR est disponible sous forme de service ou en auto-hébergement pour les organisations souhaitant conserver leurs données sensibles dans leur propre infrastructure."
  }
]