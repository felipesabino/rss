name: Build and Test with Ollama

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:

jobs:
  build-and-test:
    runs-on: ubuntu-latest
    steps:
    - name: setup-ollama
      uses: ai-action/setup-ollama@v1.1.3

    - name: Run LLM
      run: ollama run llama3.2 "" # https://github.com/ollama/ollama/blob/main/docs/faq.md#how-can-i-preload-a-model-into-ollama-to-get-faster-response-times

    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '20'
        cache: 'npm'

    - name: Install dependencies
      run: npm ci

    - name: Run tests with Ollama as OpenAI replacement
      env:
        OPENAI_API_KEY: "dummy-key-not-used-by-ollama"
        OPENAI_API_BASE_URL: "http://localhost:11434/v1/"
        OPENAI_MODEL_NAME: llama3.2
      run: |
        npm run build
        npm run check

    - name: Run static generator
      env:
        OPENAI_API_KEY: "dummy-key-not-used-by-ollama"
        OPENAI_API_BASE_URL: "http://localhost:11434/v1/"
        OPENAI_MODEL_NAME: llama3.2
      run: |
        npm run generate